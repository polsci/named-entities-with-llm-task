{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d7db3e",
   "metadata": {},
   "source": [
    "# Named entity recognition with an LLM\n",
    "\n",
    "Geoff Ford  \n",
    "[https://geoffford.nz](https://geoffford.nz/)  \n",
    "\n",
    "Consult the [README](README.md) for detailed information and the [CHANGELOG](CHANGELOG.md) for changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dff11",
   "metadata": {},
   "source": [
    "Run this cell to install required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c330702",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0a100",
   "metadata": {},
   "source": [
    "Run the following cell to import relevant Python libraries used in this notebook and set the logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a89c27",
   "metadata": {},
   "source": [
    "The [README](README.md) file discusses how to generate an OpenRouter API key. Configure the key by running this cell ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572823a2",
   "metadata": {},
   "source": [
    "The following cell contains a function to query Open Router and generate LLM text. Just run it to make the function available. Change it if you know what you are doing.\n",
    "\n",
    "Note: you can use API endpoints compatible with the OpenAI completion endpoint, but you will need to specify the relevant `api_url` and specify an `api_url` to `query_llm` calls. For example, if you have software to run LLMs locally, like [Ollama](https://ollama.com/), you can specifying an `api_url` (e.g. `http://127.0.0.1:11434/v1/chat/completions`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bfb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt:str, # prompt to send to LLM\n",
    "            model: str, # model name e.g. google/gemma-2-9b-it:free\n",
    "            system_prompt: str = None, # system prompt to send to LLM\n",
    "            max_tokens: int = 2048, # maximum number of tokens to generate (includes prompt tokens)\n",
    "            response_format: str = None, # response format: json or None\n",
    "            temperature: float = None, # temperature for sampling\n",
    "            api_url: str = None # OpenAI completion endpoint compatible API to query, defaults to OpenRouter's API \n",
    "            ) -> str: # generated text from LLM call\n",
    "    \"\"\" Query LLM with prompt \"\"\"\n",
    "\n",
    "    if api_url is None or api_url.strip() == '':\n",
    "        api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    if OPENROUTER_API_KEY is None:\n",
    "        logging.error(\"OPENROUTER_API_KEY not set. Not querying llm.\")\n",
    "        return None\n",
    "    api_key = OPENROUTER_API_KEY\n",
    "    \n",
    "    if prompt.strip() == '':\n",
    "        logging.error('No prompt provided. Not querying llm.')\n",
    "        return None\n",
    "    \n",
    "    messages = []\n",
    "    if system_prompt is not None and system_prompt.strip() != '':\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    request_data = {\n",
    "                \"model\": model, \n",
    "                \"messages\": messages,\n",
    "                'max_tokens': max_tokens\n",
    "                    }\n",
    "\n",
    "    if temperature is not None:\n",
    "        request_data['temperature'] = temperature\n",
    "    \n",
    "    if response_format == \"json\":\n",
    "        request_data['response_format'] = {\"type\": \"json_object\"}\n",
    "        \n",
    "    text = None\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "            },\n",
    "            data=json.dumps(request_data)\n",
    "            )\n",
    "        response.raise_for_status() \n",
    "        text = response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error querying LLM: {e}\")\n",
    "        print(response.json())\n",
    "        raise\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619145b",
   "metadata": {},
   "source": [
    "## Set the model (and a note about OpenRouter free models)\n",
    "\n",
    "When using OpenRouter's free models it is possible that specific models will be unavailable at times. If you get errors querying OpenRouter you can look up the [message or error codes in their documentation](https://openrouter.ai/docs). The `query_llm` function will raise errors when the API responds with an error code or if the JSON data returned by the API does not include generated content. If you get an error when using a free model, it is likely that this is temporary. Changing to [another free model](https://openrouter.ai/models?max_price=0) will typically resolve the issue. Look for models with '(free)' in their name. The next cell is where you can set the model to use for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2237b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'meta-llama/llama-3-8b-instruct:free'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb64994",
   "metadata": {},
   "source": [
    "## Task: Named entity extraction with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae861b",
   "metadata": {},
   "source": [
    "### Note about the temperature setting\n",
    "\n",
    "The only parameter implemented in the `query_llm` function is `temperature` (([video](https://www.youtube.com/watch?v=ezgqHnWvua8)). Feel free to implement [other parameters available in OpenRouter's API](https://openrouter.ai/docs/parameters) if you are confident doing this, but this is not expected for class activities or supported.  \n",
    "\n",
    "Lower temperature values give similiar or identical responses. Higher values produce more varied responses.\n",
    "The default value of temperature is 1.0. It can vary between 0.0 and 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae595880",
   "metadata": {},
   "source": [
    "### Come up with a system_prompt and provide the input text\n",
    "\n",
    "Your task is to come up with an appropriate system prompt to extract named entities from text supplied in the prompt. \n",
    "\n",
    "The system prompt currently only specifies to generate JSON data and has no instruction about named entity recognition. The system prompt can be improved by adding further instructions, including specifying the types of named entities to return. The suggested JSON format can also be altered, but the code in the next cells relies on returning a JSON array with `entities` as the key.\n",
    "\n",
    "The lab task asks you to run named entity extraction on specific texts. Paste the text into the prompt.\n",
    "\n",
    "For an application like named entity recognition, the temperature should be set to a low value to ensure less variation in responses.\n",
    "\n",
    "You can change the model above, but use one of the OpenRouter free models. Getting good quality output is more challenging with a smaller model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e725fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "\n",
    "Only reply with JSON data.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "    \"entities\": [\n",
    "    ]\n",
    "}\n",
    "\n",
    "INPUT:\n",
    "'''\n",
    "\n",
    "prompt = '''\n",
    "This text should be replaced with the texts you want to extract entities from. For now it is just a placeholder.\n",
    "Here is a sentence to extract entities from: Hi there, I am Joe and I live in Christchurch. \n",
    "'''\n",
    "\n",
    "max_tokens = 10000\n",
    "\n",
    "temperature = 0.1\n",
    "\n",
    "response_format = 'json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9de68",
   "metadata": {},
   "source": [
    "Running the next cell queries the API and outputs a response. If the response is valid JSON you should get a list of entities. Otherwise, you will see the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa03a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_llm(prompt, model, system_prompt, max_tokens, response_format = response_format, temperature = temperature)\n",
    "\n",
    "try:\n",
    "    json_data = json.loads(response)\n",
    "    for i, entity in enumerate(json_data['entities']):\n",
    "        print(entity)\n",
    "except json.JSONDecodeError as e:\n",
    "    print('Error decoding JSON. Try regenerating. A lower temperature value may help.')\n",
    "    print('Here is the response that was returned:')\n",
    "    print(response)\n",
    "except KeyError as e:\n",
    "    print('Error decoding JSON. An array with key \"entities\" is expected, but was not part of the JSON output.')\n",
    "    print('Here is the response that was returned:')\n",
    "    print(response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b9705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
